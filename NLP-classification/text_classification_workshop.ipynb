{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–µ–º–∏–Ω–∞—Ä –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "\n",
    "–í —ç—Ç–æ–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –∑–∞–Ω—è—Ç–∏–∏ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã (—Ç–≤–∏—Ç—ã) –Ω–∞ **–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ** –∏ **–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ** —Å –ø–æ–º–æ—â—å—é —Ç—Ä–µ—Ö —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤:\n",
    "\n",
    "1. **TF-IDF** ‚Äî –ø—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤\n",
    "2. **Word2Vec** ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–º—ã—Å–ª–æ–≤—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤\n",
    "3. **BERT** ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å, –ø–æ–Ω–∏–º–∞—é—â–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –≤–∑—è—Ç—ã —Å [Kaggle](https://www.kaggle.com/datasets/maximsuvorov/rutweetcorp?select=twitter_corpus.csv)\n",
    "\n",
    "## –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è\n",
    "\n",
    "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É (Shift+Enter)\n",
    "2. –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
    "3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "\n",
    "‚ö†Ô∏è **–í–ê–ñ–ù–û:** –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É –æ–¥–∏–Ω —Ä–∞–∑ –≤ –Ω–∞—á–∞–ª–µ —Ä–∞–±–æ—Ç—ã. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.\n",
    "\n",
    "**–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –æ—à–∏–±–∫–∏ —Å tokenizers:**\n",
    "1. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ: `pip install --upgrade pip wheel setuptools`\n",
    "2. –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ TF-IDF –∏ Word2Vec (–æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –±–µ–∑ transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.13/site-packages (26.0.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (82.0.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.13/site-packages (0.46.3)\n",
      "Requirement already satisfied: packaging>=24.0 in ./.venv/lib/python3.13/site-packages (from wheel) (26.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (3.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: gensim in ./.venv/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (4.67.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in ./.venv/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.10.0)\n",
      "Requirement already satisfied: transformers==5.1.0 in ./.venv/lib/python3.13/site-packages (5.1.0)\n",
      "Requirement already satisfied: accelerate==1.12.0 in ./.venv/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers==5.1.0) (4.67.3)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from accelerate==1.12.0) (7.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.28.1)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.15.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.16.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.13/site-packages (from typer-slim->transformers==5.1.0) (8.3.1)\n",
      "\n",
      "‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n"
     ]
    }
   ],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ (–∑–∞–ø—É—Å–∫–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑!)\n",
    "# –û–±–Ω–æ–≤–ª—è–µ–º pip –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–±–æ—Ä–∫–∏\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤)\n",
    "!pip install pandas scikit-learn gensim nltk tqdm\n",
    "\n",
    "# PyTorch –∏ Transformers (–Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è BERT)\n",
    "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏ –¥–ª—è Python 3.13+\n",
    "# –ï—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –º–æ–∂–µ—Ç–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏\n",
    "# –∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å TF-IDF –∏ Word2Vec\n",
    "!pip install torch transformers==5.1.0 accelerate==1.12.0\n",
    "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–µ—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç): –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ BERT –±–µ–∑ Trainer (—Å–º. –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é —è—á–µ–π–∫—É –Ω–∏–∂–µ)\n",
    "\n",
    "print(\"\\n‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ç–≤–∏—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ttext', 'label']].to_csv('classification_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 226834 —Å–æ–æ–±—â–µ–Ω–∏–π\n",
      "   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: 114911 (50.7%)\n",
      "   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 111923 (49.3%)\n",
      "\n",
      "üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
      "   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: 181467 —Å–æ–æ–±—â–µ–Ω–∏–π\n",
      "   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: 45367 —Å–æ–æ–±—â–µ–Ω–∏–π\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "df = pd.read_csv('classification_data.csv')\n",
    "\n",
    "# –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∫—É\n",
    "texts = df['ttext'].astype(str).tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å–æ–æ–±—â–µ–Ω–∏–π\")\n",
    "print(f\"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(labels) - sum(labels)} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} —Å–æ–æ–±—â–µ–Ω–∏–π\")\n",
    "print(f\"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} —Å–æ–æ–±—â–µ–Ω–∏–π\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 3: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "–û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\n",
      "‚úÖ –¢–µ–∫—Å—Ç—ã –æ—á–∏—â–µ–Ω—ã!\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "  @snumri —Ç—ã –∂–µ –≤ —Ü–µ–Ω—Ç–Ω–µ—Ä–µ —Ä–∞–±–æ—Ç–∞–µ—à—å!! –º–Ω–µ –æ—Ç –∫–æ—Ñ–µ–±–∏–Ω–∞ –¥–æ –ö. –µ—Ö–∞—Ç—å –≤–æ–æ–±—â–µ –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ. –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∫–∞–∫ ((\n",
      "\n",
      "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:\n",
      "  —Ç—ã –∂–µ –≤ —Ü–µ–Ω—Ç–Ω–µ—Ä–µ —Ä–∞–±–æ—Ç–∞–µ—à—å!! –º–Ω–µ –æ—Ç –∫–æ—Ñ–µ–±–∏–Ω–∞ –¥–æ –∫. –µ—Ö–∞—Ç—å –≤–æ–æ–±—â–µ –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ. –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∫–∞–∫ ((\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = text.lower()\n",
    "    # –£–¥–∞–ª—è–µ–º URL\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # –£–¥–∞–ª—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"üßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
    "X_train_clean = [clean_text(text) for text in X_train]\n",
    "X_test_clean = [clean_text(text) for text in X_test]\n",
    "\n",
    "print(\"‚úÖ –¢–µ–∫—Å—Ç—ã –æ—á–∏—â–µ–Ω—ã!\")\n",
    "print(f\"\\n–ü—Ä–∏–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\\n  {X_train[0]}\")\n",
    "print(f\"\\n–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:\\n  {X_train_clean[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî• –í–´–ë–ï–†–ò–¢–ï –ú–ï–¢–û–î –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø üî•\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ **–û–î–ù–£** –∏–∑ —Ç—Ä–µ—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —Å–µ–∫—Ü–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫–æ–π –º–µ—Ç–æ–¥ —Ö–æ—Ç–∏—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å:\n",
    "\n",
    "- **–ú–µ—Ç–æ–¥ 1: TF-IDF** (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π, ~1 –º–∏–Ω—É—Ç–∞)\n",
    "- **–ú–µ—Ç–æ–¥ 2: Word2Vec** (—Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å, ~5-10 –º–∏–Ω—É—Ç)\n",
    "- **–ú–µ—Ç–æ–¥ 3: BERT** (—Å–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π, ~20-30 –º–∏–Ω—É—Ç, —Ç—Ä–µ–±—É–µ—Ç transformers)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå –ú–µ—Ç–æ–¥ 1: TF-IDF + –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:** –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç, –∫–∞–∫ —á–∞—Å—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ, –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.\n",
    "\n",
    "**–ü–ª—é—Å—ã:**\n",
    "- –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π\n",
    "- –ü—Ä–æ—Å—Ç–æ–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è\n",
    "- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏\n",
    "\n",
    "**–ú–∏–Ω—É—Å—ã:**\n",
    "- –ù–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª —Å–ª–æ–≤\n",
    "- –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ TF-IDF...\n",
      "\n",
      "üìù –®–∞–≥ 1/3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã...\n",
      "   ‚úÖ –°–æ–∑–¥–∞–Ω–æ 5000 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "\n",
      "üéì –®–∞–≥ 2/3: –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...\n",
      "   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\n",
      "\n",
      "üìä –®–∞–≥ 3/3: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\n",
      "\n",
      "============================================================\n",
      "üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø TF-IDF\n",
      "============================================================\n",
      "‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 3.45 —Å–µ–∫—É–Ω–¥\n",
      "\n",
      "üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    0.7295 (72.95%)\n",
      "üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   0.7223 (72.23%)\n",
      "üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       0.7573 (75.73%)\n",
      "üéØ F1-Score:               0.7394 (73.94%)\n",
      "\n",
      "üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π       0.74      0.70      0.72     22385\n",
      "  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π       0.72      0.76      0.74     22982\n",
      "\n",
      "    accuracy                           0.73     45367\n",
      "   macro avg       0.73      0.73      0.73     45367\n",
      "weighted avg       0.73      0.73      0.73     45367\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ TF-IDF...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
    "print(\"üìù –®–∞–≥ 1/3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_clean)\n",
    "X_test_tfidf = vectorizer.transform(X_test_clean)\n",
    "print(f\"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {X_train_tfidf.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "\n",
    "# –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "print(\"\\nüéì –®–∞–≥ 2/3: –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...\")\n",
    "model_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "print(\"   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n",
    "\n",
    "# –®–∞–≥ 3: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "print(\"\\nüìä –®–∞–≥ 3/3: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\")\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø TF-IDF\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥\\n\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "precision = precision_score(y_test, y_pred_tfidf)\n",
    "recall = recall_score(y_test, y_pred_tfidf)\n",
    "f1 = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"üéØ F1-Score:               {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\\n\")\n",
    "print(classification_report(y_test, y_pred_tfidf, target_names=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ –ó–ê–î–ê–ù–ò–ï 1: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ TF-IDF\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: max_features=5000, ngram=(1, 2)\n",
      "\n",
      "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
      "   Accuracy:  0.7295\n",
      "   F1-Score:  0.7394\n",
      "\n",
      "üí° –ó–∞–ø–∏—à–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã!\n"
     ]
    }
   ],
   "source": [
    "max_features_exp = 5000  # –ü–æ–ø—Ä–æ–±—É–π—Ç–µ: 1000, 3000, 10000\n",
    "ngram_exp = (1, 2)  # –ü–æ–ø—Ä–æ–±—É–π—Ç–µ: (1,1), (1,2), (1,3)\n",
    "\n",
    "print(f\"üî¨ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: max_features={max_features_exp}, ngram={ngram_exp}\\n\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features_exp, ngram_range=ngram_exp)\n",
    "X_tr_exp = vectorizer.fit_transform(X_train_clean)\n",
    "X_te_exp = vectorizer.transform(X_test_clean)\n",
    "\n",
    "model_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_tfidf.fit(X_tr_exp, y_train)\n",
    "y_pred_exp = model_tfidf.predict(X_te_exp)\n",
    "\n",
    "print(\"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "print(f\"   Accuracy:  {accuracy_score(y_test, y_pred_exp):.4f}\")\n",
    "print(f\"   F1-Score:  {f1_score(y_test, y_pred_exp):.4f}\")\n",
    "print(\"\\nüí° –ó–∞–ø–∏—à–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç —Å–∞–º—ã–º–∏ –≤–∞–∂–Ω—ã–º–∏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòä –¢–æ–ø-10 –ü–û–ó–ò–¢–ò–í–ù–´–• —Å–ª–æ–≤:\n",
      "\n",
      "   dd                   (9.947)\n",
      "   ddd                  (7.749)\n",
      "   dddd                 (4.757)\n",
      "   –ø—Ä–∏—è—Ç–Ω–æ              (4.428)\n",
      "   xd                   (4.121)\n",
      "   –∞—Ö–∞—Ö–∞                (3.912)\n",
      "   –∞—Ö–∞—Ö–∞—Ö               (3.872)\n",
      "   –∞—Ö–∞—Ö                 (3.825)\n",
      "   —Ä–∂—É                  (3.815)\n",
      "   —Å–ø–∞—Å–∏–±–æ              (3.685)\n",
      "\n",
      "üòû –¢–æ–ø-10 –ù–ï–ì–ê–¢–ò–í–ù–´–• —Å–ª–æ–≤:\n",
      "\n",
      "   o_o                  (-9.059)\n",
      "   –æ_–æ                  (-7.174)\n",
      "   –æ–±–∏–¥–Ω–æ               (-6.924)\n",
      "   cio_optimal          (-6.789)\n",
      "   to_over_kill         (-6.303)\n",
      "   –ø–µ—á–∞–ª—å–Ω–æ             (-6.032)\n",
      "   –≥—Ä—É—Å—Ç–Ω–æ              (-5.769)\n",
      "   do_or_die_xxx        (-5.166)\n",
      "   –ø–µ—á–∞–ª—å               (-4.992)\n",
      "   99                   (-4.780)\n",
      "\n",
      "‚ùì –í–æ–ø—Ä–æ—Å—ã:\n",
      "   - –°–æ–≥–ª–∞—Å–Ω—ã –ª–∏ –≤—ã —Å —ç—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏?\n",
      "   - –ö–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞—Å —É–¥–∏–≤–∏–ª–∏?\n"
     ]
    }
   ],
   "source": [
    "# –¢–æ–ø-10 —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "import numpy as np\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "coefs = model_tfidf.coef_[0]\n",
    "\n",
    "top_pos = np.argsort(coefs)[-10:]\n",
    "top_neg = np.argsort(coefs)[:10]\n",
    "\n",
    "print(\"üòä –¢–æ–ø-10 –ü–û–ó–ò–¢–ò–í–ù–´–• —Å–ª–æ–≤:\\n\")\n",
    "for idx in reversed(top_pos):\n",
    "    print(f\"   {features[idx]:20s} ({coefs[idx]:.3f})\")\n",
    "\n",
    "print(\"\\nüòû –¢–æ–ø-10 –ù–ï–ì–ê–¢–ò–í–ù–´–• —Å–ª–æ–≤:\\n\")\n",
    "for idx in top_neg:\n",
    "    print(f\"   {features[idx]:20s} ({coefs[idx]:.3f})\")\n",
    "\n",
    "print(\"\\n‚ùì –í–æ–ø—Ä–æ—Å—ã:\")\n",
    "print(\"   - –°–æ–≥–ª–∞—Å–Ω—ã –ª–∏ –≤—ã —Å —ç—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏?\")\n",
    "print(\"   - –ö–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞—Å —É–¥–∏–≤–∏–ª–∏?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå –ú–µ—Ç–æ–¥ 2: Word2Vec + –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:** –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤, –≥–¥–µ –ø–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É.\n",
    "\n",
    "**–ü–ª—é—Å—ã:**\n",
    "- –ü–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª–æ–≤—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤\n",
    "- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏\n",
    "\n",
    "**–ú–∏–Ω—É—Å—ã:**\n",
    "- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –£—Å—Ä–µ–¥–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec...\n",
      "\n",
      "üìù –®–∞–≥ 1/4: –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Å–ª–æ–≤–∞...\n",
      "   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 181467 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "\n",
      "üéì –®–∞–≥ 2/4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Word2Vec –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\n",
      "   üìä –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç 93672 —Å–ª–æ–≤\n",
      "\n",
      "üîÑ –®–∞–≥ 3/4: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã...\n",
      "   ‚úÖ –°–æ–∑–¥–∞–Ω–æ 100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
      "\n",
      "üéì –®–∞–≥ 4/4: –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...\n",
      "   ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\n",
      "\n",
      "üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\n",
      "\n",
      "============================================================\n",
      "üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø WORD2VEC\n",
      "============================================================\n",
      "‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 8.26 —Å–µ–∫—É–Ω–¥\n",
      "\n",
      "üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    0.7669 (76.69%)\n",
      "üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   0.7624 (76.24%)\n",
      "üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       0.7842 (78.42%)\n",
      "üéØ F1-Score:               0.7731 (77.31%)\n",
      "\n",
      "üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π       0.77      0.75      0.76     22385\n",
      "  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π       0.76      0.78      0.77     22982\n",
      "\n",
      "    accuracy                           0.77     45367\n",
      "   macro avg       0.77      0.77      0.77     45367\n",
      "weighted avg       0.77      0.77      0.77     45367\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# –®–∞–≥ 1: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "print(\"üìù –®–∞–≥ 1/4: –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Å–ª–æ–≤–∞...\")\n",
    "X_train_tokens = [text.split() for text in X_train_clean]\n",
    "X_test_tokens = [text.split() for text in X_test_clean]\n",
    "print(f\"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(X_train_tokens)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "\n",
    "# –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ Word2Vec\n",
    "print(\"\\nüéì –®–∞–≥ 2/4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=X_train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "print(\"   ‚úÖ Word2Vec –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n",
    "print(f\"   üìä –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(w2v_model.wv)} —Å–ª–æ–≤\")\n",
    "\n",
    "# –®–∞–≥ 3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "print(\"\\nüîÑ –®–∞–≥ 3/4: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã...\")\n",
    "\n",
    "def text_to_vector(tokens, model):\n",
    "    \"\"\"–£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –≤—Å–µ—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ\"\"\"\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_w2v = np.array([text_to_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([text_to_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "print(f\"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {X_train_w2v.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\")\n",
    "\n",
    "# –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "print(\"\\nüéì –®–∞–≥ 4/4: –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...\")\n",
    "model_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_w2v.fit(X_train_w2v, y_train)\n",
    "print(\"   ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "print(\"\\nüìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\")\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø WORD2VEC\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥\\n\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_w2v)\n",
    "precision = precision_score(y_test, y_pred_w2v)\n",
    "recall = recall_score(y_test, y_pred_w2v)\n",
    "f1 = f1_score(y_test, y_pred_w2v)\n",
    "\n",
    "print(f\"üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"üéØ F1-Score:               {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\\n\")\n",
    "print(classification_report(y_test, y_pred_w2v, target_names=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ –ó–ê–î–ê–ù–ò–ï 2: –ò—Å—Å–ª–µ–¥—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è\n",
    "\n",
    "Word2Vec —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤. –î–∞–≤–∞–π—Ç–µ –Ω–∞–π–¥–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤:\n",
      "\n",
      "üìå '—Ö–æ—Ä–æ—à–æ':\n",
      "   —Ö–æ—Ä–æ—à–æ,         (0.750)\n",
      "   –≤–µ—Å–µ–ª–æ          (0.680)\n",
      "   –∫—Ä—É—Ç–æ           (0.676)\n",
      "   –∫–ª–∞—Å—Å–Ω–æ         (0.673)\n",
      "   —Ö–æ—Ä–æ—à–æ.         (0.657)\n",
      "\n",
      "üìå '–ø–ª–æ—Ö–æ':\n",
      "   –ø–ª–æ—Ö–æ,          (0.796)\n",
      "   –≥—Ä—É—Å—Ç–Ω–æ         (0.786)\n",
      "   –±–æ–ª—å–Ω–æ          (0.753)\n",
      "   —Ö—Ä–µ–Ω–æ–≤–æ         (0.744)\n",
      "   –ø–ª–æ—Ö–æ.          (0.735)\n",
      "\n",
      "üìå '–ª—é–±–ª—é':\n",
      "   –æ–±–æ–∂–∞—é          (0.781)\n",
      "   –ª—é–±–ª—é,          (0.702)\n",
      "   –ª—é–±–∏–ª–∞          (0.684)\n",
      "   –ª—é–±–ª—é!          (0.683)\n",
      "   –æ–±–Ω–∏–º–∞—é         (0.643)\n",
      "\n",
      "üìå '—Å—á–∞—Å—Ç—å–µ':\n",
      "   —á—É–¥–æ            (0.769)\n",
      "   –∞—Ö—É–µ–Ω–Ω–æ–µ        (0.760)\n",
      "   —Å–º–µ—à–Ω–æ–µ         (0.751)\n",
      "   –∫–ª–∞—Å—Å–Ω–æ–µ        (0.751)\n",
      "   –∫—Ä–∞—Å–∏–≤–æ–µ        (0.746)\n",
      "\n",
      "üí° TODO: –ó–∞–º–µ–Ω–∏—Ç–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å–≤–æ–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞!\n",
      "\n",
      "‚ùì –í–æ–ø—Ä–æ—Å—ã:\n",
      "   - –°–æ–≥–ª–∞—Å–Ω—ã –ª–∏ –≤—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Å–ª–æ–≤–∞–º–∏?\n",
      "   - –ö–∞–∫–∏–µ –ø–∞—Ä—ã —É–¥–∏–≤–∏–ª–∏?\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞–π–¥–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞\n",
    "test_words = ['—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–ª—é–±–ª—é', '—Å—á–∞—Å—Ç—å–µ']\n",
    "\n",
    "print(\"üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤:\\n\")\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
    "        print(f\"üìå '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"   {sim_word:15s} ({score:.3f})\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\\n\")\n",
    "\n",
    "print(\"üí° TODO: –ó–∞–º–µ–Ω–∏—Ç–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å–≤–æ–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞!\")\n",
    "print(\"\\n‚ùì –í–æ–ø—Ä–æ—Å—ã:\")\n",
    "print(\"   - –°–æ–≥–ª–∞—Å–Ω—ã –ª–∏ –≤—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Å–ª–æ–≤–∞–º–∏?\")\n",
    "print(\"   - –ö–∞–∫–∏–µ –ø–∞—Ä—ã —É–¥–∏–≤–∏–ª–∏?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå –ú–µ—Ç–æ–¥ 3: BERT (Transformer)\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª—É–±–æ–∫—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–æ–≤ –∏ –∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏.\n",
    "\n",
    "**–ü–ª—é—Å—ã:**\n",
    "- –°–∞–º–æ–µ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n",
    "- –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤\n",
    "- –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π state-of-the-art –º–µ—Ç–æ–¥\n",
    "\n",
    "**–ú–∏–Ω—É—Å—ã:**\n",
    "- –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥\n",
    "- –¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏\n",
    "- –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ transformers\n",
    "\n",
    "**‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï:** \n",
    "- –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 20-30 –º–∏–Ω—É—Ç!\n",
    "- –ï—Å–ª–∏ transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ TF-IDF –∏–ª–∏ Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å!\\n\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå –û–®–ò–ë–ö–ê: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!\")\n",
    "    print(\"\\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å:\")\n",
    "    print(\"  !pip install --upgrade pip wheel setuptools\")\n",
    "    print(\"  !pip install torch transformers\")\n",
    "    print(\"\\n–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ TF-IDF –∏ Word2Vec –≤–º–µ—Å—Ç–æ BERT.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT...\n",
      "\n",
      "‚ö†Ô∏è  –≠—Ç–æ –∑–∞–π–º–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è! –ù–∞–±–µ—Ä–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏—è...\n",
      "\n",
      "üìù –®–∞–≥ 1/4: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 1856.72it/s, Materializing param=bert.pooler.dense.weight]                              \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cointegrated/rubert-tiny2\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "bert.embeddings.position_ids               | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\n",
      "\n",
      "üîÑ –®–∞–≥ 2/4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ 181467 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "\n",
      "‚öôÔ∏è  –®–∞–≥ 3/4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...\n",
      "   ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ—Ç–æ–≤—ã!\n",
      "\n",
      "üéì –®–∞–≥ 4/4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è, —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11342' max='11342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11342/11342 02:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.553241</td>\n",
       "      <td>0.541981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "\n",
      "üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø BERT\n",
      "============================================================\n",
      "‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 174.91 —Å–µ–∫—É–Ω–¥ (2.9 –º–∏–Ω—É—Ç)\n",
      "\n",
      "üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    0.7649 (76.49%)\n",
      "üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   0.7826 (78.26%)\n",
      "üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       0.7421 (74.21%)\n",
      "üéØ F1-Score:               0.7618 (76.18%)\n",
      "\n",
      "üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π       0.75      0.79      0.77     22385\n",
      "  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π       0.78      0.74      0.76     22982\n",
      "\n",
      "    accuracy                           0.76     45367\n",
      "   macro avg       0.77      0.77      0.76     45367\n",
      "weighted avg       0.77      0.76      0.76     45367\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT...\\n\")\n",
    "print(\"‚ö†Ô∏è  –≠—Ç–æ –∑–∞–π–º–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è! –ù–∞–±–µ—Ä–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏—è...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–ª–µ–≥—á–µ–Ω–Ω—É—é —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "print(\"üìù –®–∞–≥ 1/4: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")\n",
    "print(\"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n",
    "\n",
    "for name, param in model_bert.named_parameters():\n",
    "     if name.startswith(\"cls\") or name.startswith(\"bert\"): \n",
    "        param.requires_grad = False\n",
    "\n",
    "# –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\nüîÑ –®–∞–≥ 2/4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)\n",
    "sample_size = min(200000, len(X_train_clean))  \n",
    "X_train_sample = X_train_clean[:sample_size]\n",
    "y_train_sample = y_train[:sample_size]\n",
    "\n",
    "train_dataset = TextDataset(X_train_sample, y_train_sample, tokenizer)\n",
    "test_dataset = TextDataset(X_test_clean, y_test, tokenizer)\n",
    "print(f\"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(train_dataset)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "\n",
    "# –®–∞–≥ 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "print(\"\\n‚öôÔ∏è  –®–∞–≥ 3/4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # –í—Å–µ–≥–æ –æ–¥–Ω–∞ —ç–ø–æ—Ö–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: eval_strategy –≤–º–µ—Å—Ç–æ evaluation_strategy\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "print(\"   ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ—Ç–æ–≤—ã!\")\n",
    "\n",
    "# –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ\n",
    "print(\"\\nüéì –®–∞–≥ 4/4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è, —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º)...\\n\")\n",
    "trainer.train()\n",
    "print(\"\\n   ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "print(\"\\nüìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\")\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred_bert = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø BERT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥ ({elapsed_time/60:.1f} –º–∏–Ω—É—Ç)\\n\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_bert)\n",
    "precision = precision_score(y_test, y_pred_bert)\n",
    "recall = recall_score(y_test, y_pred_bert)\n",
    "f1 = f1_score(y_test, y_pred_bert)\n",
    "\n",
    "print(f\"üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"üéØ Precision (–ü—Ä–µ—Ü–∏–∑–∏—è):   {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"üéØ Recall (–ü–æ–ª–Ω–æ—Ç–∞):       {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"üéØ F1-Score:               {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\\n\")\n",
    "print(classification_report(y_test, y_pred_bert, target_names=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéâ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ BERT –Ω–∞ —Å–≤–æ–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö\n",
    "\n",
    "–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –µ—ë –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT:\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –ö–∞–∫–æ–π –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ª–∏—á–Ω–æ–µ :)\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –£–∂–∞—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞, –≤—Å–µ –ø–ª–æ—Ö–æ :(\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: üòû –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –û–ø—è—Ç—å –≤—Å–µ —Å–ª–æ–º–∞–ª–æ—Å—å, –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: üòû –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –ö–∞–∫–æ–π –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ª–∏—á–Ω–æ–µ\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ —Å–≤–æ–∏!)\n",
    "test_examples = [\n",
    "    \"–ö–∞–∫–æ–π –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ª–∏—á–Ω–æ–µ :)\",\n",
    "    \"–£–∂–∞—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞, –≤—Å–µ –ø–ª–æ—Ö–æ :(\",\n",
    "    \"–û–ø—è—Ç—å –≤—Å–µ —Å–ª–æ–º–∞–ª–æ—Å—å, –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...\",\n",
    "    \"–ö–∞–∫–æ–π –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ª–∏—á–Ω–æ–µ\",\n",
    "]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å!)\n",
    "def predict_sentiment(texts):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    cleaned = [clean_text(text) for text in texts]\n",
    "    \n",
    "    if torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\") \n",
    "    encodings = tokenizer(cleaned, truncation=True, padding=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**encodings)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).to('cpu').numpy()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(f\"üîÆ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT:\\n\")\n",
    "predictions = predict_sentiment(test_examples)\n",
    "\n",
    "for text, pred in zip(test_examples, predictions):\n",
    "    sentiment = \"üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\" if pred == 1 else \"üòû –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\"\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ –ó–ê–î–ê–ù–ò–ï 3: –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–≤–æ–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö\n",
    "\n",
    "–ü—Ä–∏–¥—É–º–∞–π—Ç–µ —Å–≤–æ–∏ —Ç–≤–∏—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Å–µ —Ç—Ä–∏ –º–æ–¥–µ–ª–∏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï\n",
      "============================================================\n",
      "\n",
      "üìù 1. –°–µ–≥–æ–¥–Ω—è –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ–Ω—å!\n",
      "   üí≠ –û–∂–∏–¥–∞—é: üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
      "   üìä TF-IDF:   üòä ‚úÖ\n",
      "   üåê Word2Vec: üòä ‚úÖ\n",
      "   ü§ñ BERT:     üòä ‚úÖ\n",
      "\n",
      "üìù 2. –í—Å–µ —É–∂–∞—Å–Ω–æ :(\n",
      "   üí≠ –û–∂–∏–¥–∞—é: üòû –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n",
      "   üìä TF-IDF:   üòû ‚úÖ\n",
      "   üåê Word2Vec: üòû ‚úÖ\n",
      "   ü§ñ BERT:     üòû ‚úÖ\n",
      "\n",
      "üìù 3. –ù—É –Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ...\n",
      "   üí≠ –û–∂–∏–¥–∞—é: üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
      "   üìä TF-IDF:   üòä ‚úÖ\n",
      "   üåê Word2Vec: üòû ‚ùå\n",
      "   ü§ñ BERT:     üòä ‚úÖ\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚ùì –í–æ–ø—Ä–æ—Å—ã:\n",
      "   - –í—Å–µ –ª–∏ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã?\n",
      "   - –ù–∞ –∫–∞–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –æ–Ω–∏ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è?\n",
      "   - –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å —Ç–æ—á–Ω–µ–µ?\n"
     ]
    }
   ],
   "source": [
    "my_tweets = [\n",
    "    \"–°–µ–≥–æ–¥–Ω—è –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ–Ω—å!\",  # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
    "    \"–í—Å–µ —É–∂–∞—Å–Ω–æ :(\",  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n",
    "    \"–ù—É –Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ...\",  # –ù–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π\n",
    "    # TODO: –î–æ–±–∞–≤—å—Ç–µ –µ—â–µ 5-7 —Å–≤–æ–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "]\n",
    "\n",
    "# TODO: –ù–µ –∑–∞–±—É–¥—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è —Å–≤–æ–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "my_expected = [1, 0, 1]  # 1 = –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, 0 = –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n",
    "\n",
    "print(\"üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï\\n\" + \"=\"*60)\n",
    "\n",
    "for i, tweet in enumerate(my_tweets):\n",
    "    print(f\"\\nüìù {i+1}. {tweet}\")\n",
    "    print(f\"   üí≠ –û–∂–∏–¥–∞—é: {'üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π' if my_expected[i]==1 else 'üòû –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π'}\")\n",
    "    \n",
    "    cleaned = clean_text(tweet)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        p = model_tfidf.predict(vectorizer.transform([cleaned]))[0]\n",
    "        match = '‚úÖ' if p == my_expected[i] else '‚ùå'\n",
    "        print(f\"   üìä TF-IDF:   {'üòä' if p==1 else 'üòû'} {match}\")\n",
    "    except:\n",
    "        print(\"   üìä TF-IDF:   –Ω–µ –æ–±—É—á–µ–Ω–∞\")\n",
    "    \n",
    "    # Word2Vec\n",
    "    try:\n",
    "        v = text_to_vector(cleaned.split(), w2v_model).reshape(1,-1)\n",
    "        p = model_w2v.predict(v)[0]\n",
    "        match = '‚úÖ' if p == my_expected[i] else '‚ùå'\n",
    "        print(f\"   üåê Word2Vec: {'üòä' if p==1 else 'üòû'} {match}\")\n",
    "    except:\n",
    "        print(\"   üåê Word2Vec: –Ω–µ –æ–±—É—á–µ–Ω–∞\")\n",
    "    \n",
    "    # BERT\n",
    "    try:    \n",
    "        if torch.backends.mps.is_built():\n",
    "            device = torch.device(\"mps\") \n",
    "\n",
    "        encodings = tokenizer([cleaned], truncation=True, padding=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(**encodings)\n",
    "            p = torch.argmax(outputs.logits, dim=1).to('cpu').numpy()\n",
    "        match = '‚úÖ' if p == my_expected[i] else '‚ùå'\n",
    "        print(f\"   ü§ñ BERT:     {'üòä' if p==1 else 'üòû'} {match}\")\n",
    "    except:\n",
    "        print(\"   ü§ñ BERT:     –Ω–µ –æ–±—É—á–µ–Ω–∞\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚ùì –í–æ–ø—Ä–æ—Å—ã:\")\n",
    "print(\"   - –í—Å–µ –ª–∏ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã?\")\n",
    "print(\"   - –ù–∞ –∫–∞–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –æ–Ω–∏ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è?\")\n",
    "print(\"   - –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å —Ç–æ—á–Ω–µ–µ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä –ó–ê–î–ê–ù–ò–ï 4: –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "\n",
    "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤:\n",
    "\n",
    "| –ú–µ—Ç–æ–¥ | Accuracy | F1-Score | –í—Ä–µ–º—è | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |\n",
    "|-------|----------|----------|-------|-------|--------|\n",
    "| TF-IDF | ? | ? | ? | ? | ? |\n",
    "| Word2Vec | ? | ? | ? | ? | ? |\n",
    "| BERT | ? | ? | ? | ? | ? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "\n",
    "**Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å)** ‚Äî –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "- –§–æ—Ä–º—É–ª–∞: (–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è) / (–í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)\n",
    "- –ü—Ä–∏–º–µ—Ä: –ï—Å–ª–∏ –∏–∑ 100 —Ç–≤–∏—Ç–æ–≤ –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ 85, —Ç–æ accuracy = 0.85 (85%)\n",
    "\n",
    "**Precision (–ü—Ä–µ—Ü–∏–∑–∏—è)** ‚Äî –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –Ω–∞–∑–≤–∞–ª–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º–∏, –∫–∞–∫–∞—è –¥–æ–ª—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è\n",
    "- –§–æ—Ä–º—É–ª–∞: (–ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ) / (–í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞–∫ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ)\n",
    "- –í–∞–∂–Ω–∞, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å –ª–æ–∂–Ω—ã—Ö —Ç—Ä–µ–≤–æ–≥\n",
    "\n",
    "**Recall (–ü–æ–ª–Ω–æ—Ç–∞)** ‚Äî –∏–∑ –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∫–∞–∫—É—é –¥–æ–ª—é –º–æ–¥–µ–ª—å –Ω–∞—à–ª–∞\n",
    "- –§–æ—Ä–º—É–ª–∞: (–ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ) / (–í—Å–µ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ)\n",
    "- –í–∞–∂–Ω–∞, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "**F1-Score** ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ –º–µ–∂–¥—É Precision –∏ Recall\n",
    "- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–Ω–æ—Ç–æ–π\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ –ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "1. –û–±—É—á–∏—Ç–µ –≤—Å–µ —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ (TF-IDF, Word2Vec, BERT) –∏–ª–∏ —Ö–æ—Ç—è –±—ã –¥–≤–µ, –µ—Å–ª–∏ BERT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è\n",
    "2. –ó–∞–ø–∏—à–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü—É:\n",
    "\n",
    "| –ú–µ—Ç–æ–¥ | Accuracy | Precision | Recall | F1-Score | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è |\n",
    "|-------|----------|-----------|--------|----------|----------------|\n",
    "| TF-IDF | ? | ? | ? | ? | ? |\n",
    "| Word2Vec | ? | ? | ? | ? | ? |\n",
    "| BERT | ? | ? | ? | ? | ? |\n",
    "\n",
    "3. –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:\n",
    "   - –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç?\n",
    "   - –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –±—ã—Å—Ç—Ä–µ–µ –≤—Å–µ–≥–æ?\n",
    "   - –ö–∞–∫–æ–π –º–µ—Ç–æ–¥ –≤—ã –±—ã –≤—ã–±—Ä–∞–ª–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–æ—á–µ–º—É?\n",
    "   - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–Ω—é—é —è—á–µ–π–∫—É. –í—Å–µ –ª–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç?\n",
    "\n",
    "---\n",
    "\n",
    "## üí° –°–æ–≤–µ—Ç—ã –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º\n",
    "\n",
    "**–ï—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è transformers –∏–ª–∏ accelerate:**\n",
    "```bash\n",
    "# –í —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:\n",
    "pip install --upgrade pip wheel setuptools\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "pip install transformers>=4.45.0 accelerate>=0.35.0\n",
    "```\n",
    "\n",
    "**–î–ª—è Python 3.13+:** –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ—Ä—Å–∏–∏ `transformers>=4.45.0` –∏ `accelerate>=0.35.0`\n",
    "\n",
    "**–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ 'keep_torch_compile' –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ BERT:**\n",
    "\n",
    "–≠—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–π. –£ –≤–∞—Å –µ—Å—Ç—å –î–í–ê —Ä–µ—à–µ–Ω–∏—è:\n",
    "\n",
    "1. **–û–±–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):**\n",
    "```bash\n",
    "pip install --upgrade transformers accelerate\n",
    "```\n",
    "\n",
    "2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é —è—á–µ–π–∫—É —Å BERT** (–±–µ–∑ Trainer API):\n",
    "   - –í notebook –µ—Å—Ç—å –¥–≤–µ —è—á–µ–π–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERT\n",
    "   - –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥)\n",
    "   - –û–Ω–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç accelerate –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ\n",
    "\n",
    "**–ï—Å–ª–∏ –º–∞–ª–æ –ø–∞–º—è—Ç–∏ –¥–ª—è BERT:**\n",
    "- –£–º–µ–Ω—å—à–∏—Ç–µ `sample_size` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 5000)\n",
    "- –£–º–µ–Ω—å—à–∏—Ç–µ `per_device_train_batch_size` / `batch_size` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 8)\n",
    "- –£–º–µ–Ω—å—à–∏—Ç–µ `num_train_epochs` –¥–æ 1\n",
    "\n",
    "**–ï—Å–ª–∏ BERT –≤–æ–æ–±—â–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "- –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ! TF-IDF –∏ Word2Vec –¥–∞—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (72-75% accuracy)\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "- BERT —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É\n",
    "- –î–ª—è —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª–µ–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
